Thanks! I’ll look into alternative distance or similarity measures for MIP problems that go beyond Kendall tau and better correlate with solver performance like Gurobi solve times.

I’ll explore measures based on structural properties of MIPs — such as sparsity patterns, variable-constraint graph metrics, presolve behavior predictors, or features known to impact branching or presolving — especially ones computable *before* solving. I’ll get back to you shortly with what I find.

# Introduction  
Small changes in a mixed-integer programming (MIP) model’s formulation can lead to surprisingly large differences in solver performance. In practice, simply reordering variables or constraints can double or halve Gurobi’s solve time ([What is performance variability? – Gurobi Help Center](https://support.gurobi.com/hc/en-us/articles/26989876720657-What-is-performance-variability#:~:text=Imagine%20that%20you%20run%20your,a%20consequence%20of%20%2022)). This *performance variability* occurs because different orderings or formulations cause the solver to take different solution paths (due to tie-breaking, presolve limits, etc.) ([Order of variables & constraints vs. performance – Gurobi Help Center](https://support.gurobi.com/hc/en-us/community/posts/360048065332-Order-of-variables-constraints-vs-performance#:~:text=There%20are%20several%20reasons%20why,the%20order%20of%20variables%20and)) ([Order of variables & constraints vs. performance – Gurobi Help Center](https://support.gurobi.com/hc/en-us/community/posts/360048065332-Order-of-variables-constraints-vs-performance#:~:text=set%20of%20conditions%20does%20not,running%20time%20quite%20a%20lot)). Currently, the solver variability is being estimated by the Kendall tau distance between permutations of problem elements (i.e. how different two orderings of variables and constraints are). However, Kendall tau is a purely positional measure – it doesn’t capture deeper structural characteristics of the MIP. To better predict when one formulation will be more “solver-friendly” (i.e. faster to solve) than another, we need *structural or mathematical distance measures* that correlate with actual solve time differences. These measures should be computable *a priori* (before actually solving the MIP) and ideally reflect aspects of the model that influence the branch-and-bound search, linear relaxations, and presolve effectiveness. In this report, we survey several alternative metrics – from graph-based features of the constraint matrix to coefficient disorder and relaxation quality – and discuss how they might predict solver performance variability. We also touch on instance features from literature and domain-specific considerations. The goal is to identify measures that change when the formulation (or even just the input ordering) changes, and that those changes align with observed differences in solve time. 

**Solver Variability Example:** Gurobi’s documentation illustrates how dramatic the impact of constraint ordering can be. In one example, five versions of the same model (Glass4 from MIPLIB) were created by cyclically permuting the first constraint to the end. The solve times varied roughly tenfold – from about 4 seconds up to 45 seconds – even though these models are mathematically identical ([What is performance variability? – Gurobi Help Center](https://support.gurobi.com/hc/en-us/articles/26989876720657-What-is-performance-variability#:~:text=,87)). This unpredictability is attributed to subtle effects on presolve and heuristics. A given ordering might be “lucky” – for example, exposing an easy reduction early or guiding the heuristic to a good incumbent – while another ordering obscures those advantages ([Order of variables & constraints vs. performance – Gurobi Help Center](https://support.gurobi.com/hc/en-us/community/posts/360048065332-Order-of-variables-constraints-vs-performance#:~:text=set%20of%20conditions%20does%20not,running%20time%20quite%20a%20lot)). Because Kendall tau distance treats all permutation differences uniformly, it cannot tell which permutation changes preserve a solver-friendly structure and which disrupt it. We need metrics that quantify properties like “are related constraints grouped together?” or “does this formulation have a tighter LP bound?” – properties with known influence on solver behavior. In the following sections, we explore categories of such metrics. For each, we discuss *what* to measure, *why* it matters for solver performance, and how one might use it to compare two formulations or instance orderings. Tables and references to recent research are provided to summarize and compare these measures.

# Graph-Based Structural Metrics  
One intuitive way to characterize a MIP’s formulation is by its **interaction graph**. We can represent the constraint matrix as a bipartite graph with a node for each constraint and each variable, and edges linking a variable to the constraints in which it appears.  ([ResearchGate](https://www.researchgate.net/figure/figure/The-bipartite-graph-representation-of-an-MILP_fig3_383364875/download)) *Illustration of a bipartite constraint-variable graph for a MILP.* From this graph (or its one-mode projections), we can derive several structural metrics:

- **Constraint Graph Treewidth:** If we construct a graph of constraints (nodes are constraints, and an edge connects two constraints that share a variable), the *treewidth* of this graph is a measure of how close the graph is to a tree. A low treewidth means the constraints can be “almost” arranged in a tree structure, indicating a sparse, loosely coupled system. Empirically, MIPs with small treewidth or nearly tree-structured interactions tend to be easier, because dynamic programming or efficient decomposition methods become viable. In the extreme case of treewidth 1 (a pure tree structure), the MIP might be solved by sequential or DP methods rather than exhaustive search. Conversely, a high treewidth (e.g. a large clique in the constraint graph) indicates a highly interconnected model where many variables and constraints interact simultaneously – a situation that typically leads to large branch-and-bound trees. While computing treewidth exactly is NP-hard in general, even approximate measures or upper bounds could be useful as a *similarity metric*: two formulations might be considered distant if one introduces significantly more fill-in or connectivity than the other. Prior studies in parameterized complexity have noted the connection between treewidth and IP complexity ([[PDF] Treewidth and Integer Programming](https://coral.ise.lehigh.edu/mip-2006/talks/Koster.pdf#:~:text=Mixed%20Integer%20Programming%20workshop,%E2%96%AB%20Treewidth%20by%20Integer)), and specialized algorithms exploit small-treewidth structures. Thus, if one permutation or formulation yields a constraint graph with a lower treewidth (for instance, by clustering related constraints together as a nearly acyclic component), it is likely more solver-friendly.

- **Node Degree and Centrality:** Features like the degree of nodes (in the bipartite graph) or centrality measures can indicate “linchpin” variables or constraints. For example, a variable that appears in almost every constraint (a very high-degree variable) couples the entire model – branching on it or handling it in the LP basis might be pivotal and potentially problematic. A formulation where such a “hub” variable is present or where a few constraints connect to many variables might be harder to solve, as it prevents decomposition of the problem. By contrast, if the bipartite graph can be easily partitioned into communities (clusters) with only a few cross-links, the solver’s presolve or decomposition routines might isolate subproblems. **Betweenness centrality** of a variable (measuring how often it lies on shortest paths between other variables in the constraint graph) could serve as a proxy for how critical that variable is in linking different parts of the model. High centrality variables or constraints often indicate a core bottleneck structure (like a capacity constraint touching many subcomponents). If one formulation reduces the centrality of certain nodes (for instance, by splitting a big constraint into smaller ones, or by ordering such that related items are contiguous), it might correlate with improved solver performance. These graph metrics are domain-agnostic and can be computed from the matrix structure alone. As a distance measure, one could define a vector of graph features (maximum degree, average degree, clustering coefficient, etc.) for each formulation and measure a distance (Euclidean or otherwise) between these vectors ([](https://or.rwth-aachen.de/files/research/publications/image-based-similarity-MIPs.pdf#:~:text=this%20discussion%2C%20the%20novel%20features,properties%20of%20the%20CCM%2C%205)) ([](https://or.rwth-aachen.de/files/research/publications/image-based-similarity-MIPs.pdf#:~:text=in%20Definition%20%281%29%2C%20respectively%29,instances%20according%20to%20this%20metric)). In fact, the MIPLIB 2017 comparison index used 110 instance features – including graph connectivity measures – and defines similarity as the Euclidean distance in that feature space ([](https://or.rwth-aachen.de/files/research/publications/image-based-similarity-MIPs.pdf#:~:text=in%20Definition%20%281%29%2C%20respectively%29,instances%20according%20to%20this%20metric)). If two permutations yield different graph feature values (even though the underlying logical graph is identical, the *ordering* might affect which substructures are considered “contiguous” – we’ll address this under ordering-specific metrics), those differences could be used to predict which ordering is more favorable.

- **Community Detection and Clustering:** Real-world MIP models often have an underlying block structure – e.g. a model might consist of several loosely coupled submodules. One way to quantify this is via community detection on the bipartite graph or the constraint graph. A formulation that naturally reveals clusters (for instance, constraints can be grouped into clusters that share few variables with other clusters) is generally easier to solve or can be exploited by methods like decomposition or parallel solves. Solvers like Gurobi do some automatic decomposition detection; also, the Generic Column Generation tool (GCG) attempts to find block structure in MIPs. **Clustering coefficient** (how tightly knit the neighborhood of a node is) is another related metric – a high average clustering might imply overlapping constraints that could be reformulated or indicate redundancy. For comparing two formulations, one could report the number of distinct communities or the modularity of the bipartite graph as a feature. If one permutation yields a clearer separation of communities (i.e. higher modularity, meaning variables and constraints can be partitioned into well-separated groups), that may hint that the solver can isolate parts of the problem more effectively (leading to fewer global branching decisions). Domain-specific knowledge can refine this: e.g. in a supply chain MIP, you might expect separate communities for production, transportation, and inventory constraints; if a formulation intermixes them (low modularity), the solver might thrash between decisions, whereas a formulation that keeps them somewhat modular (possibly by ordering them in contiguous blocks) could allow the solver’s preprocessing to exploit that structure. In summary, **graph-based metrics** provide a structural “fingerprint” of the MIP. They are largely invariant under pure re-orderings of variables and constraints (since the graph connectivity doesn’t change), so on their own they might not explain performance differences between two permutations of the *same* instance. However, if we broaden the notion of “distance” to include how an ordering aligns with the graph structure, we can define ordering-dependent graph metrics. For example, one could measure the *bandwidth* or profile of the adjacency matrix (discussed next) or define an “ordering quality” metric: e.g. sum of distances between connected constraints in the ordering. A low sum-of-distances (meaning connected constraints are placed close together in the input order) could be a predictor of good performance, aligning with expert advice to group related constraints and variables ([Order of variables & constraints vs. performance – Gurobi Help Center](https://support.gurobi.com/hc/en-us/community/posts/360048065332-Order-of-variables-constraints-vs-performance#:~:text=IMHO%2C%20no%2C%20there%20is%20not,course%20these%20are%20loose%20statements)). In the next section, we delve into such matrix-oriented measures.

# Matrix Sparsity and Ordering Structure  
Beyond abstract graphs, the concrete pattern of the constraint coefficient matrix provides rich metrics. **Sparsity or density** measures are among the simplest: count the number of nonzero coefficients relative to matrix size. Very sparse formulations (each constraint involves only a few variables) tend to be easier, all else equal, because they decompose the decision space. A dense constraint (one involving a large fraction of the variables) acts like a “global” coupling – for example, a budget constraint touching every decision variable can make the LP relaxation fully dense and can propagate any change everywhere. So, one basic feature is *nonzero density*: if one formulation yields a sparser matrix (perhaps by introducing auxiliary variables or separating constraints), it might solve faster. However, the raw density alone is not a perfect predictor (some sparse models are still hard if the sparse pattern forms a difficult combinatorial structure). We can refine this by looking at the **distribution of nonzeros per row and per column**. For instance, the maximum row density (max number of variables in any one constraint) is a useful indicator: a very large value here suggests at least one “big” constraint that could slow simplex iterations or branching. Similarly, a high variance in row density means some constraints are much more complex than others; the solver might spend disproportionate effort on those. If comparing two formulations of the same problem, one might have more balanced row densities (no extremely dense constraint), which could be advantageous. 

**Bandedness and Matrix Profile:** If variables and constraints are indexed in some “natural” correlated order, the coefficient matrix may exhibit a banded structure (nonzeros concentrated around a diagonal band). This often happens if the model has a time-step or spatial ordering. Solvers don’t explicitly exploit matrix bandwidth the way a linear equation solver might, but indirectly it can matter: a banded or clustered matrix often means related constraints are contiguous, which can help presolve find substitutions or cuts more locally. A measure here is the *matrix bandwidth*: after ordering, define row indices for constraints and column indices for variables; the bandwidth is the maximum distance between a nonzero’s row index and column index (after suitably normalizing, or considering some combined ordering for a square matrix representation). In a bipartite sense, we can measure how far apart in the ordering a variable and constraint that interact are. Another similar concept is the **block-diagonal structure** with permutation. If by permuting rows/columns you can almost block-diagonalize the matrix (leaving only a few off-diagonal nonzeros), that’s a sign of an exploitable structure. We could quantify the fraction of matrix elements that lie within a certain block structure or count the number of blocks identifiable. As a concrete metric, solvers or preprocessing tools can compute connected components of the bipartite graph – if the matrix actually splits into independent components, those are trivially solved separately. More often, there is one giant component, but perhaps nearly decomposed. A metric like **bridge edges count** (number of single connections between two otherwise separate parts) could indicate that removing a handful of “link” constraints would split the problem. If one formulation has fewer such link constraints (or variables), it might effectively decompose better.

**Importance of Ordering in Matrix Structure:** While the sparsity pattern (which variable connects to which constraint) is invariant to mere reordering, the *presentation* can affect solver behavior. Gurobi’s internal algorithms, while generally order-independent in theory, do have components like partial presolve and heuristic scanning that are order-sensitive ([Order of variables & constraints vs. performance – Gurobi Help Center](https://support.gurobi.com/hc/en-us/community/posts/360048065332-Order-of-variables-constraints-vs-performance#:~:text=There%20are%20several%20reasons%20why,the%20order%20of%20variables%20and)). For example, presolve might scan constraints sequentially for simplification opportunities; if related constraints are adjacent, certain substitutions or aggregations might be detected earlier. As Gurobi expert Daniel Espinoza noted, *“there is a benefit to leaving ‘related constraints’ and ‘related variables’ close together (in contiguous chunks)”* ([Order of variables & constraints vs. performance – Gurobi Help Center](https://support.gurobi.com/hc/en-us/community/posts/360048065332-Order-of-variables-constraints-vs-performance#:~:text=IMHO%2C%20no%2C%20there%20is%20not,course%20these%20are%20loose%20statements)). We can define a measure of “relatedness clustering” in an ordering. One approach is to leverage the constraint graph: for each edge (constraint A connected to constraint B via a shared variable), look at the positions of A and B in the ordering of constraints. We could take the average or maximum of |pos(A) – pos(B)| as a metric – a smaller value means constraints that share variables are near each other in the input. A low average separation might correlate with better performance, since it aligns with the advice to cluster. This metric essentially measures how well the ordering *follows the graph structure*. Kendall tau, in contrast, would treat any reordering equally, but if a permutation drastically increases the average distance between linked constraints (essentially “scrambling” the matrix), one might expect solver performance to drop. In summary, useful **matrix-level features** include: total nonzeros, max/min/avg nonzeros per row (and per column), and ordering-dependent ones like bandwidth or average connectivity distance. Many of these were included in the MIPLIB instance features ([](https://or.rwth-aachen.de/files/research/publications/image-based-similarity-MIPs.pdf#:~:text=In%20Gleixner%20et%20al,values%20within%20an%20average%20constraint)). For instance, the MIPLIB 2017 feature set defines a *matrix density* group and a *matrix coefficient* group (we discuss coefficients shortly) and even reports a *block matrix score*. As a practical guideline, if you are comparing two formulations or permutations, you might create a table of these matrix metrics for each and observe which formulation has more “solver-friendly” values (e.g. sparser, lower max row density, more clustered). Those differences can serve as a *distance measure* (for example, one could sum the absolute differences in a set of normalized metrics to get a single distance score). The table below summarizes various metrics and their expected relationship with solve time:

| **Measure Category**       | **Examples of Metrics**                            | **Relation to Solve Time**                                                                        |
|----------------------------|----------------------------------------------------|---------------------------------------------------------------------------------------------------|
| **Graph-based structure**  | – Treewidth of constraint graph<br>– Degree centrality of variables/constraints<br>– Community clustering (modularity) | Lower treewidth or identifiable clusters indicate simpler structure that solvers can exploit. Highly connected graphs (large cliques) often lead to more branching. High-degree “hub” variables increase coupling and can make the search harder. In formulations with clear communities (subnetworks), solvers may effectively decompose or focus search, improving performance. |
| **Matrix sparsity & order**| – Nonzero density (% of matrix filled)<br>– Nonzeros per constraint (min/avg/max)<br>– Matrix bandwidth under given ordering<br>– Block-diagonal or clustered structure score | Sparser matrices generally yield easier LPs (fewer fill-ins in factorization) and simpler branching. A very dense constraint matrix can slow down every node LP solve. If an ordering yields a narrow bandwidth or clear blocks of nonzeros, the solver’s presolve and linear algebra can perform better by working on localized subsets ([Order of variables & constraints vs. performance – Gurobi Help Center](https://support.gurobi.com/hc/en-us/community/posts/360048065332-Order-of-variables-constraints-vs-performance#:~:text=IMHO%2C%20no%2C%20there%20is%20not,course%20these%20are%20loose%20statements)). Grouping related constraints/vars (reflected in low bandwidth or separation) tends to be beneficial. |
| **Coefficient statistics** | – Coefficient range per constraint (*dynamism*)<br>– Mean and variance of coefficients<br>– Coefficient entropy or disorder<br>– Count of big-“M” constants | Formulations with extreme coefficient ratios (high *row dynamism*) can be numerically challenging and lead to instability ([MIPLIB 2017: Data-Driven Compilation of the 6th Mixed-Integer Programming Library](https://www.dei.unipd.it/~salvagni/pdf/miplib6.pdf#:~:text=The%20dynamism%20of%20a%20vector,we%20see%20this%20as%20an)). A high variance or entropy in coefficients (widely varying magnitudes or signs) often indicates lack of structure and can inhibit presolve (difficult to find cancellations or substitutions). Numerous big-M coefficients (very large constants in constraints) signal weak relaxations and potential numerical issues – they often correlate with longer solve times because the LP relaxation is loose until those constraints become active deep in the search. Keeping coefficients scaled and “tight” (low dynamism) generally makes the formulation more solver-friendly. |
| **LP relaxation features** | – LP objective value (lower bound) vs. IP optimum<br>– Integrality gap percentage<br>– Number of fractional integer vars in LP solution<br>– LP iteration count or condition number | A stronger LP relaxation (small integrality gap) usually means fewer branch-and-bound nodes are needed, as the solver’s initial bound is closer to optimal. If the LP optimum is nearly integral (few fractional values), the solver may find an integer solution quickly or prove optimality at the root. Conversely, if the LP leaves many variables fractional, the solver must branch many times. Even without solving the IP, one can use the *LP fractionality* as a feature. Additionally, an LP that solves very slowly (many simplex iterations or degeneracy) suggests the formulation might have numerical issues or degeneracy that could carry into the MILP search. |
| **Instance-specific & others** | – Variables by type (binary, integer, continuous count)<br>– Constraint types composition (e.g. % knapsack, % flow) ([](https://or.rwth-aachen.de/files/research/publications/image-based-similarity-MIPs.pdf#:~:text=2,although%20this%20early%20constraint))<br>– Symmetry indicators (identical columns or rows)<br>– Presolve reductions (vars fixed, constraints removed) | Certain structural features of instances are known to impact difficulty. For example, models with a very high proportion of binary variables or with set-partitioning constraints (common in scheduling) are often harder because they create huge combinatorial spaces. Constraint classification features (e.g. identifying knapsack constraints, network flow structure, etc.) help – if an instance is mostly network flow constraints, it’s typically easier than one with many arbitrary logical constraints. Symmetry in the formulation (e.g. many variables with identical coefficient patterns) can dramatically increase solve time by causing redundant search; measuring symmetry (such as detecting groups of variables that can be permuted without changing the constraints) can predict a need for symmetry-breaking or indicate hardness. Finally, running a *light presolve* and counting the simplifications can be very informative: if presolve (or even an analytical approximation of it) can fix many variables or remove constraints, the remaining core problem is smaller and likely faster. A formulation that yields more presolve reductions is effectively stronger. (One caveat: you’d need to actually perform presolve to measure that, but even a few rounds or heuristics could be done offline.) In summary, features used in instance classification and algorithm selection literature capture many of these aspects, and differences in these features between two formulations would be a useful distance measure ([Why do we need to measure the difficulty of mixed-integer programming problems? - Operations Research Stack Exchange](https://or.stackexchange.com/questions/3707/why-do-we-need-to-measure-the-difficulty-of-mixed-integer-programming-problems#:~:text=What%20you%20are%20looking%20for,been%20a%20topic%20also%20recently)). |

*(Table: Key categories of measures for comparing MIP formulations, with examples and their expected influence on solver runtime.)*

# Coefficient Disorder and Entropy  
Zooming in on the coefficients of the constraints, we find another set of metrics. MIP formulations can differ not just in which constraints or variables they have, but in the **numerical properties** of their coefficients. A classic metric here is the *dynamism* of a constraint (also called the coefficient range): this is the ratio of the largest coefficient to the smallest nonzero coefficient in that constraint. MIPLIB 2017 explicitly tracks “row dynamism” as a feature ([MIPLIB 2017: Data-Driven Compilation of the 6th Mixed-Integer Programming Library](https://www.dei.unipd.it/~salvagni/pdf/miplib6.pdf#:~:text=The%20dynamism%20of%20a%20vector,we%20see%20this%20as%20an)) because it matters – if a single constraint contains coefficients that differ by orders of magnitude, the constraint is numerically imbalanced. Solvers may struggle with such constraints in the simplex algorithm (leading to degeneracy or unstable pivots), and branch-and-bound might spend effort branching to satisfy the tightest part of the constraint while the looser part was never binding anyway. If one formulation simply reorders the same coefficients, the dynamism per constraint remains the same; but different formulations (say one uses a scaling factor or aggregates constraints differently) can change coefficient ranges. As a rule, **lower dynamism (after proper scaling)** is preferred. If you compare two formulations and one has consistently higher dynamism in its constraints, that one might lead to slower solves (due to numerical difficulty or needing more cuts to cover wide ranges). 

Another idea is to treat the matrix or each constraint as a distribution of values and compute an **entropy** or disorder measure. For example, if a constraint’s coefficients are all very similar (e.g. all 1’s and 2’s), that’s low entropy; if they vary wildly (e.g. {100, -3, 0.5, -200}), that’s high entropy. It’s hypothesized that high coefficient entropy makes the formulation “less structured” – solvers benefit from regularity (like all coefficients positive and of similar magnitude, which might indicate a knapsack structure amenable to specific cuts). High disorder might mean no single cutting plane or heuristic easily captures the constraint’s effect, requiring more exploration. One could compute entropy of absolute values (normalized) for each row and take an average. This is an experimental metric, but it could potentially correlate with performance differences. If you permute constraints, entropy per row stays same, but if you permute *variables* and compare two different orderings, the *column-wise* view changes: one could also compute entropy column-wise (distribution of coefficients in constraints for each variable). Invariantly, that doesn’t change with order either, but different formulations (like introducing new variables or splitting constraints) will alter these distributions.

**Sign patterns** are another aspect: Are the coefficients mostly positive, mostly negative, or mixed? Mixed-sign constraints (where some coefficients are positive, some negative) can lead to more cancellation and maybe more difficulty in reasoning (they resemble linear equations that can cancel out, possibly indicating redundant constraints or alternating tightness). A formulation that separates positive and negative contributions into separate constraints (thus each constraint has a clear inequality direction) might be easier for presolve to tighten bounds. No specific numeric metric is widely used here, but one could define a “sign entropy” (e.g. fraction of coefficients that are negative, etc.).

**Big-M constants count:** Many MIP formulations use big $M$ constraints (e.g. $M y \ge x$ to model logic). If $M$ is excessively large, those constraints are very loose until $y$ is 0/1 decided, causing a weak LP relaxation and potential numerical issues. Counting how many big-$M$ constraints (or how large those $M$ values are relative to typical coefficients) is a domain-specific but important measure. If formulation A uses a smaller $M$ (tighter logic constraint) than formulation B, A will likely have a stronger LP bound and solve faster. Thus, one could define a metric like “Sum of $M$ values” or “Max $M$” as part of the feature vector.

To summarize, coefficient-based measures focus on **numerical quality** of the formulation. They can serve to predict solver time differences especially when formulations differ in scaling or tightness. For example, if you create two formulations of a scheduling problem – one with time indexed binary variables and big $M$ constraints linking them to start times, and another with a different encoding – their graph structure might be similar, but one might have huge coefficients while the other doesn’t. The one with more balanced coefficients should be more solver-friendly. In our context of merely permuting the model, coefficient measures won’t change at all (reordering doesn’t change any coefficient’s value). So these are more relevant for *different formulations* rather than just different orderings of the same formulation. However, they are still worth noting because the user is open to domain-specific measures (and perhaps considering slightly modified formulations, not just pure permutations). 

Empirical guideline: **keep coefficients scaled and avoid extreme ratios**. If needed, apply normalization. For instance, Gurobi and other solvers recommend scaling your model so coefficients are generally within 6 orders of magnitude of each other. If two formulations model the same logic but one uses values in the millions and tiny decimals, the solver might spend time dealing with numerical precision there. Measures like the maximum absolute coefficient, the coefficient of variation of all coefficients, etc., can quantify this. In MIPLIB’s feature set, “matrix coefficients” features include the mean, min, max of coefficients, and a measure called “constraint activity range” and “rhs range” which also relate to this ([](https://or.rwth-aachen.de/files/research/publications/image-based-similarity-MIPs.pdf#:~:text=feature%20groups,values%20within%20an%20average%20constraint)).

# LP Relaxation and Formulation Strength  
One of the most telling indicators of how hard a MIP will be is the quality of its **LP relaxation**. The LP relaxation is obtained by ignoring integrality and just solving the linear program. If a formulation is “tight,” the LP optimum will be close to the true integer optimum; if it’s “loose,” there will be a big gap, necessitating many branch-and-bound nodes to close it. While solving the full LP relaxation can be time-consuming for very large instances, it is often still much faster than solving the integer problem, and yields valuable metrics. If it’s feasible to compute, **LP-based features** should be included:

- **Integrality Gap:** Compute `(IP_obj - LP_obj) / IP_obj` (or absolute difference). Of course, to get IP_obj (optimal integer objective) you’d need to solve the MILP or have prior knowledge. But for comparison between formulations of the *same* problem, you might know the optimal value (it should be identical for equivalent formulations). In such cases, the one with a smaller LP gap is expected to be easier. Even if optimal IP objective is unknown, one can sometimes use a feasible solution’s value as a stand-in. This measure directly correlates with search: e.g. if formulation X has a 5% gap at the root and formulation Y has 50% gap, Y will likely require much more branching or cutting to close that gap. 

- **Fractionality of Solution:** Count how many integer-constrained variables take fractional values in the LP solution. If nearly all binaries come out 0/1 in the LP, then the LP solution is almost integer – often the solver can round it to get a feasible solution, or at least it means integrality constraints aren’t binding strongly (maybe the problem’s structure already yields integral vertices – e.g. network flow problems with unimodular matrices do this). If a formulation yields *more* fractional variables in the relaxation than another formulation of the same problem, that’s a sign the second formulation encodes integrality more weakly. For example, the classic example is the Traveling Salesman Problem: a naive subtour elimination formulation vs. a stronger comb inequality formulation – the latter would have far fewer fractional subtour decisions at the LP. In less extreme cases, even adding redundant constraints or tightening bounds can reduce fractionality.

- **LP Objective Distribution and Basis:** The number of simplex iterations or the conditioning of the optimal basis could also matter. If one formulation’s LP is particularly challenging to solve (maybe due to degeneracy with many alternate optima, indicated by many zero reduced costs or basis flips), that could foreshadow a harder MILP solve. Metrics like **LP solve time (at root)** or basis condition number can be proxies for that. However, since we want pre-solve metrics, using LP solve time is a bit like cheating (it’s almost a mini-solve). But one could justify solving just the root LP as a diagnostic, since it’s still much cheaper than full branch-and-bound in many cases. There is research on **predicting branch-and-bound tree size from root node features** – e.g. strong branching scores, etc., but those require partial solving (branching once or a few times with strong branching to gauge the tree growth). If allowed, those can greatly enhance predictions (they effectively simulate the solver’s early steps). If not allowed, one sticks to static LP features.

Some features can be computed *very* quickly without solving the full LP: for instance, **continuous relaxations of individual constraints** or trivial bounds. Also, **dual bounds tightening**: presolve often performs LP-based bound tightening on single constraints (e.g. infer tighter variable bounds from constraints). The amount of tightening possible could be a feature – if formulation A allows a lot of bounds tightening at presolve (meaning many constraints were redundant or variables had slack that could be removed), it might have more redundancy but end up easier after presolve. Formulation B with little presolve improvement might mean it was already tight or, conversely, that nothing easy could be done and the solver must work hard in the search.

In literature, a measure called **“LP relaxation strength”** or **“node zero gap”** is commonly cited: basically the gap after the root node. A smaller root gap correlates with fewer nodes on average. Fischetti et al. (2019) and other researchers have tried to predict runtime from such features, and indeed the root gap often is one of the top predictors ([Why do we need to measure the difficulty of mixed-integer programming problems? - Operations Research Stack Exchange](https://or.stackexchange.com/questions/3707/why-do-we-need-to-measure-the-difficulty-of-mixed-integer-programming-problems#:~:text=What%20you%20are%20looking%20for,been%20a%20topic%20also%20recently)). If comparing two formulations of one problem, you can simply compute their root gaps; the one with the smaller gap is expected to solve faster in most cases. (There are exceptions, like if one formulation’s root is slow but then it finds a good heuristic solution early, etc., but generally gap is king.)

To include an **example**, consider two formulations of a facility location problem: Formulation A has an explicit constraint linking open facilities to served clients with a big-M; Formulation B uses a flow-based formulation that implicitly enforces that link. Both are correct, but Formulation B might have a much tighter LP (no big-M slack). If you compute the LP optimum for both, Formulation A might give a much lower lower bound (hence larger gap). Indeed, empirical studies show that certain extended formulations (adding flow variables, etc.) drastically tighten the LP and reduce node count even though they add more variables – those are considered “strong” formulations.

Finally, one can look at **cutting plane predictions**: if a formulation has a certain structure known to generate strong cuts (e.g. lots of knapsack constraints will generate Gomory cuts or cover cuts that close gap quickly), one could anticipate that. There isn’t an obvious *prior-to-solving* number for “how many cuts will be needed,” but one could attempt to derive something like the Chvátal rank of the initial formulation (how many rounds of cuts to get integer hull). In practice, solvers automatically generate cuts, but if the formulation is weak, they may need many rounds. Perhaps a proxy feature is **percentage of integrality constraints that are violated by the LP solution** – if that percentage is high, many cuts will be needed. If low, maybe few cuts. 

In summary, LP relaxation features blend into the algorithm’s behavior, but as pre-solve indicators of difficulty, they are very powerful. They are domain-agnostic (every MILP has an LP relaxation) but computing them may be expensive if the instance is huge. If time allows, computing the root LP of each formulation and comparing outcomes provides an excellent similarity/distance measure for solver behavior. Differences in root bound or number of fractional variables between two permutations of constraints (for an identical problem) will be zero (since the LP is identical in value), so again – these features highlight differences in *formulations*, not pure reorderings. However, even for identical LP relaxations, there’s a twist: the *solver’s approach* to the LP could differ with ordering due to pivot choices. For instance, the number of simplex iterations to solve the initial LP can vary with constraint ordering (simplex might take a different path). That could be one reason one ordering solves faster – it found the optimal LP basis quicker. So one *could* measure “LP solve iterations” for each ordering as a feature indicating how solver-friendly that ordering is to the simplex algorithm. It’s a subtle measure (and might require actually solving the LP or at least running a few simplex iterations), but it is a direct indicator of linear algebra difficulty caused by ordering. If, say, ordering X leads to 1000 iterations at root and ordering Y leads to 5000 (due to degeneracy or different pivot sequence), X might end up faster overall.

# Instance Features and Domain-Specific Measures  
The field of algorithm selection and instance analysis for MIP has developed a wide array of features that can serve as distance measures between instances. Many of those features can be leveraged here to compare formulations. We have touched on several (e.g., size, density, coefficient stats, LP gap). Let’s outline some additional ones and domain-specific considerations:

- **Basic Problem Size and Composition:** The counts of variables (and how many are binary, integer, continuous) and constraints (and their types) form a fundamental description. If two MIP instances are actually the same problem in different guise, these counts will be the same. But for different formulations of a problem, you might see, for example, one formulation has more variables (perhaps introduced auxiliary variables) but fewer constraints, etc. These trade-offs can affect solve time. In general, more variables/constraints can mean more work per node, but if they make the formulation tighter, it could still be faster overall. So as a measure, differences in $(m,n)$ (number of constraints, number of vars) can indicate formulation divergence. Solver sensitivity to size is not linear – sometimes adding a few well-chosen constraints (redundant but strengthening cuts) can drastically speed up solving. So this needs to be combined with other quality measures. Nonetheless, feature vectors typically include $n_{\text{bin}}$, $n_{\text{int}}$, $n_{\text{cont}}$, $m_{\text{ineq}}$, $m_{\text{eq}}$, etc. The Euclidean distance between such vectors for two instances could be part of an overall similarity score ([](https://or.rwth-aachen.de/files/research/publications/image-based-similarity-MIPs.pdf#:~:text=In%20Gleixner%20et%20al,values%20within%20an%20average%20constraint)).

- **Constraint Classification:** As referenced earlier, Gleixner et al. (2019) define 17 classes of linear constraints (e.g., pure network flow constraints, knapsack constraints, set packing, mixing constraints, etc.) ([](https://or.rwth-aachen.de/files/research/publications/image-based-similarity-MIPs.pdf#:~:text=2,although%20this%20early%20constraint)). An instance can be described by the percentage of its constraints falling into each class. This gives a sort of “structural signature.” If two formulations of a problem use very different modeling approaches, their constraint class distributions may differ. For example, one formulation might have many “indicator constraints” (binary logic using big-M), another might replace those with equivalent linear constraints or extra variables which are more like flow constraints. The presence of certain constraint types correlates with difficulty: **Set covering/partitioning** constraints (like those in crew scheduling or vehicle routing) are known to produce large search trees because they are highly combinatorial and symmetric. **Knapsack** constraints produce a lot of cuts (cover cuts) and can be challenging if there are many of them. **Flow** or network constraints often imply a totally unimodular structure, which is easy for the solver (LP gives integral solution). So if one formulation converts a general integer constraint into a network flow substructure, that’s a big win – measurable by saying “it now has X% network constraints vs 0% before.” As a practical measure: one can attempt to automatically detect structures (some solvers and tools do this). GCG, for example, tries to detect if an instance contains an embedded network or assignment problem. So, a distance measure could be defined in terms of recognized structures: e.g., a vector like (has network flow: yes/no, has big cliques of binary vars: yes/no, # of knapsack constraints, etc.). This is more categorical, but a stark difference (like one formulation has a network flow structure identified while another doesn’t) can foreshadow big performance differences.

- **Symmetry and Redundancy:** Two pernicious properties that make MIPs hard are *symmetry* (many equivalent solutions due to interchangeable variables) and *redundancy* (constraints that don’t cut off new feasible region but confuse the solver or blow up the matrix). As mentioned, symmetry can be detected by analyzing if variables or constraints can be permuted without changing the model. There are graph-based algorithms to find symmetries (e.g., nauty on the constraint graph). The output might be something like: there are 10 variables in one orbit (meaning they are symmetric). If a formulation has large symmetry or many interchangeable parts, the solver might need to explore essentially the same solution many times. A simple metric: the size of the largest symmetry group or the number of symmetric variables. A lower number is better. If one formulation breaks symmetry (perhaps by adding symmetry-breaking constraints or just by modeling in a way that inherently distinguishes the variables), it should solve faster. **Redundancy** can be measured by checking linear dependence among constraints or whether removing a constraint changes the feasible region. Full redundancy check is expensive (requires LP solves), but heuristics exist (e.g., if one constraint is a multiple or sum of others, easy to detect). Presolve will remove redundancies it finds; thus *the number of constraints removed by presolve* is a direct measure. If formulation A had 50 constraints removed and formulation B only 5, A had more redundant info. Redundant constraints can sometimes slow the solver (extra work in LP), though sometimes they help by guiding cuts. But usually, fewer but stronger constraints are preferred. So we might interpret a high redundancy count as a sign of a less clean formulation. That said, if both formulations are logically equivalent, they should have similar fundamental redundancy; differences might come if one formulation explicitly included additional redundant constraints as cutting planes.

- **Heuristic “solver-friendly” scores:** Some research has trained machine learning models to predict runtime or node count from static features ([Why do we need to measure the difficulty of mixed-integer programming problems? - Operations Research Stack Exchange](https://or.stackexchange.com/questions/3707/why-do-we-need-to-measure-the-difficulty-of-mixed-integer-programming-problems#:~:text=What%20you%20are%20looking%20for,been%20a%20topic%20also%20recently)). While the exact models are complex, one can glean which features are most predictive. Common ones include: LP gap, number of constraints, ratio of constraints to variables, and certain dispersion metrics (like how many constraints per variable, etc.). For SAT problems, a famous predictor is clause-to-variable ratio for hardness. In MILP, no single ratio is as predictive across the board, but combinations matter. If the user is open to using instance-feature literature, one approach is to take a set of known hard and easy instances, fit a regression or classification (some literature uses e.g. decision trees on features), and then use that model as a “score” for new instances. In our context, if you have a particular solver and problem domain, you could possibly train a model on small variations (random permutations, etc.) to see which metrics move with runtime. This might reveal, for example, that “bandwidth” had a higher correlation than Kendall tau. This is more involved, but it’s a path to *learn* an effective similarity measure rather than manually defining one.

- **Domain-specific parameters:** Finally, consider metrics tied to the real meaning of the problem. For example, in scheduling problems, the number of time periods or the tightness of deadlines can hugely affect difficulty. In MILP formulations, this might translate to how many period-indexed variables or how binding the capacity constraints are. In a network design MIP, the density of the network or the value of demands can impact which cuts become active. While these are not generic metrics, if one knows the problem domain, one can include them. For instance, if your MIPs are all, say, knapsack problems with different item counts and capacities, then obviously “number of items” is a feature and maybe “capacity utilization” (capacity / sum of item weights) indicates how constrained the problem is (a capacity around 50% of total weight tends to be hardest because it’s a medium tight knapsack). Such measures would correlate with solve times for that domain and thus serve as a predictive feature. 

Domain-specific features often make the difference in algorithm selection; they explain variance that generic features can’t. If the user’s problems come from a particular application, it’s worth researching specific papers in that area (e.g., “MIP formulation for scheduling – which formulation is stronger?” or “features predicting difficulty in vehicle routing MILPs”). For example, in the SAT domain, features like horn clause ratio or variance of clause length are used; analogously, in MILP one might use “variance of RHS values” or “density of cost vector” etc., if relevant.

# Comparative Discussion and Implementation Guidelines  
We have outlined a variety of metrics: graph connectivity, matrix sparsity, coefficient entropy, LP relaxation strength, and more. Each captures a different aspect of formulation “quality.” **No single metric is likely to perfectly predict solve time** on its own – rather, a combination gives a clearer picture. For instance, a formulation might have a great LP bound (small gap) but terrible coefficient scaling; or it might have sparse structure but huge symmetry. In practice, these factors can trade off. So one recommendation is to construct a **feature vector** that includes one or two metrics from each category above, then use a statistical or machine learning approach to correlate that with observed solve times. This essentially treats the problem as a regression: you could even attempt a linear model saying `predicted_time = a*(density) + b*(gap) + c*(dynamism) + ...`. The coefficients would tell you which features matter most. If that’s too heavy, even qualitatively checking which formulation dominates in multiple categories can guide your intuition (e.g., Formulation X has better clustering and lower dynamism, but a slightly worse LP gap – likely X is still better overall unless the LP gap difference is huge).

When comparing **permutations of the same instance** specifically, many features won’t change (size, coefficient stats, integrality gap – all invariant). The differences come from how the solver’s *algorithm* interacts with the ordering. Measures that can vary with ordering include those related to ordering of constraints/vars (bandwidth, grouping) and those related to algorithmic behavior (LP iteration count, presolve effectiveness given ordering). One pragmatic approach: run Gurobi (or your solver) on a *limited basis* on each permutation to collect some quick stats: for example, run 1 second of presolve and see how many constraints/vars are removed, or run the root node only. These short runs give dynamic metrics that reflect the ordering. You could even use Gurobi’s logs (it often prints how many presolve reductions happened, etc.) as data. That said, the question asks for measures computable prior to solving – implying we want to avoid actual runs. In that case, focusing on the structural ones is appropriate.

**Recent Research:** One recent line of work by Steever et al. uses an *image-based similarity* approach where they turn the constraint matrix into a bitmap image and use an autoencoder to extract features, then measure Euclidean distance in that feature space as a similarity score ([](https://or.rwth-aachen.de/files/research/publications/image-based-similarity-MIPs.pdf#:~:text=this%20discussion%2C%20the%20novel%20features,properties%20of%20the%20CCM%2C%205)). Interestingly, this method is domain-agnostic and can capture subtle structural similarities. It effectively automates the feature extraction (the autoencoder might be capturing patterns akin to those we described manually). Such an approach could be applied to measure distance between two MIP formulations: feed both constraint matrices through the feature extractor and see how far apart they are in the learned feature space. While this is sophisticated, it might be overkill for just permutations. Still, it’s worth noting as *recent research on instance similarity*. Another relevant area is the study of **solver sensitivity**. Researchers have looked at how changing random seeds affects performance variability ([What is performance variability? – Gurobi Help Center](https://support.gurobi.com/hc/en-us/articles/26989876720657-What-is-performance-variability#:~:text=Imagine%20that%20you%20run%20your,a%20consequence%20of%20%2022)) ([What is performance variability? – Gurobi Help Center](https://support.gurobi.com/hc/en-us/articles/26989876720657-What-is-performance-variability#:~:text=,87)) and how one can mitigate that by running multiple solves. But for our purpose (predict differences), one could simulate a bit of randomness to see if an ordering consistently outperforms another over several random seed runs, which could reduce noise in measurement.

**Empirical Findings:** To the extent available, empirical studies often highlight a few key features: the root LP gap is typically the single best predictor of runtime (when varying instances broadly) ([Why do we need to measure the difficulty of mixed-integer programming problems? - Operations Research Stack Exchange](https://or.stackexchange.com/questions/3707/why-do-we-need-to-measure-the-difficulty-of-mixed-integer-programming-problems#:~:text=What%20you%20are%20looking%20for,been%20a%20topic%20also%20recently)). In the case of formulation variants, often the formulation with additional auxiliary variables (making the problem bigger but tighter) wins out because of reduced gap and better structure. For example, a well-known result in formulation research (Vielma 2017) is that adding extra dimensions to the formulation (like a higher-dimensional polyhedron) can *exponentially* reduce required branching. Those differences would be seen in metrics like treewidth (maybe higher because more variables, ironically) but dramatically seen in LP gap (zero gap if the formulation is ideal). So depending on what differences we expect between formulations, choose metrics accordingly. If the main differences are just ordering, focus on those clustering and ordering metrics. If formulations differ by the presence of certain constraints or variables (like one has additional cuts), then include metrics for those (like count of cuts, or tighter bounds).

In terms of **implementation**: many of the static metrics (graph degrees, density, etc.) can be computed in time linear or quadratic in the input size by scanning the matrix. Graph algorithms for centrality or community might be higher order (e.g., betweenness centrality is $O(N \cdot E)$ typically), but for moderate size problems those are fine. Computing treewidth is hard, but one can use heuristics (there are tools that attempt treewidth approximations for graphs with a few hundred nodes). Coefficient stats are trivial to compute with a single pass. LP relaxation requires using an LP solver – perhaps that’s acceptable if done offline. Presolve simulation would require coding some presolve logic or again leveraging the solver’s presolve in a limited way. A practical approach is to use existing libraries: e.g., the [Feature-Based Algorithm Selection] paper by *Georges et al. (2018)* provides an open-source code to extract 399 static features from a MIP instance (they used SCIP to do it) ([](https://opus4.kobv.de/opus4-zib/files/6836/main.pdf#:~:text=Features%20extracted%20in%20such%20a,basis%20for%20our%20static%20feature)) ([](https://opus4.kobv.de/opus4-zib/files/6836/main.pdf#:~:text=we%20also%20extract%20static%20features,features%20for%20each%20problem%20instance)). One could feed the two instances (two orderings) into that feature extractor and see if any of the computed features differ. Admittedly, if it’s literally the same constraints just permuted, most of those features won’t differ except ones that depend on presolve (since presolve might behave slightly differently with order). But it might capture, for example, a difference in how fast SCIP’s presolve solved the LP or something. 

To wrap up, the **most relevant measures for permutation sensitivity** are likely: **(a)** constraint/variable grouping metrics (to quantify “relatedness” clustering), **(b)** presolve outcome metrics (like % reduction achieved), and **(c)** any observed difference in root node performance (LP iterations or cuts added). For broader formulation differences, incorporate **(d)** LP gap, **(e)** treewidth or connectivity, **(f)** dynamism, and **(g)** known hard structure indicators (like symmetry count or knapsack count). By combining these, you can define a distance or similarity measure that is far more predictive than raw Kendall tau. For example, you might end up with a weighted formula: `Distance(formA, formB) = w1*|gapA-gapB| + w2*|bandwidthA-bandwidthB| + w3*|dynamismA-dynamismB| + ...` where weights $w_i$ can be tuned. Or simply report them as a set: “Formulation A has treewidth 10 vs B’s 18, LP gap 2% vs 10%, etc.” which qualitatively indicates A should be easier. 

In conclusion, the “solver-friendliness” of a MIP formulation is a multifaceted concept. Kendall tau on ordering doesn’t capture it because it ignores *why* order matters. By looking at graph structure (how intertwined the model is), matrix characteristics (how sparse and well-arranged it is), coefficient patterns (how well-scaled and structured), relaxation tightness, and specific instance structures (like symmetry or known hard substructures), we can gain a much clearer picture. These metrics not only serve to compare instances but also give hints on *improving* formulations – e.g., if you find high symmetry, you might add symmetry-breaking constraints; if you find a huge LP gap, you consider adding cuts or variables to tighten it. The references and examples provided give a starting point for implementing these measures. For instance, MIPLIB’s feature set and the image-based similarity approach ([](https://or.rwth-aachen.de/files/research/publications/image-based-similarity-MIPs.pdf#:~:text=In%20Gleixner%20et%20al,values%20within%20an%20average%20constraint)) ([](https://or.rwth-aachen.de/files/research/publications/image-based-similarity-MIPs.pdf#:~:text=this%20discussion%2C%20the%20novel%20features,properties%20of%20the%20CCM%2C%205)) are concrete resources that demonstrate how a wide range of features can be used to compare and cluster MIP instances. By leveraging such techniques, one can define a tailored similarity score that correlates with solver time differences more strongly than permutation distance alone. Ultimately, a combination of structural insight and empirical calibration will yield the best predictor for solver variability in your specific context. 

**References:**

- Gurobi documentation on performance variability (small model changes vs. runtime) ([What is performance variability? – Gurobi Help Center](https://support.gurobi.com/hc/en-us/articles/26989876720657-What-is-performance-variability#:~:text=Imagine%20that%20you%20run%20your,a%20consequence%20of%20%2022)) ([What is performance variability? – Gurobi Help Center](https://support.gurobi.com/hc/en-us/articles/26989876720657-What-is-performance-variability#:~:text=,87)).  
- Gurobi community forum discussion on variable/constraint order effects ([Order of variables & constraints vs. performance – Gurobi Help Center](https://support.gurobi.com/hc/en-us/community/posts/360048065332-Order-of-variables-constraints-vs-performance#:~:text=There%20are%20several%20reasons%20why,the%20order%20of%20variables%20and)) ([Order of variables & constraints vs. performance – Gurobi Help Center](https://support.gurobi.com/hc/en-us/community/posts/360048065332-Order-of-variables-constraints-vs-performance#:~:text=set%20of%20conditions%20does%20not,running%20time%20quite%20a%20lot)) ([Order of variables & constraints vs. performance – Gurobi Help Center](https://support.gurobi.com/hc/en-us/community/posts/360048065332-Order-of-variables-constraints-vs-performance#:~:text=IMHO%2C%20no%2C%20there%20is%20not,course%20these%20are%20loose%20statements)).  
- MIPLIB 2017 instance features and classification of constraints ([](https://or.rwth-aachen.de/files/research/publications/image-based-similarity-MIPs.pdf#:~:text=In%20Gleixner%20et%20al,values%20within%20an%20average%20constraint)) ([](https://or.rwth-aachen.de/files/research/publications/image-based-similarity-MIPs.pdf#:~:text=2,although%20this%20early%20constraint)).  
- Definition of coefficient dynamism and its impact on numerical difficulty ([MIPLIB 2017: Data-Driven Compilation of the 6th Mixed-Integer Programming Library](https://www.dei.unipd.it/~salvagni/pdf/miplib6.pdf#:~:text=The%20dynamism%20of%20a%20vector,we%20see%20this%20as%20an)).  
- OR literature on predicting MIP runtime from instance features ([Why do we need to measure the difficulty of mixed-integer programming problems? - Operations Research Stack Exchange](https://or.stackexchange.com/questions/3707/why-do-we-need-to-measure-the-difficulty-of-mixed-integer-programming-problems#:~:text=What%20you%20are%20looking%20for,been%20a%20topic%20also%20recently)).  
- Image-based structural similarity for MIPs (feature vector comparison) ([](https://or.rwth-aachen.de/files/research/publications/image-based-similarity-MIPs.pdf#:~:text=this%20discussion%2C%20the%20novel%20features,properties%20of%20the%20CCM%2C%205)).